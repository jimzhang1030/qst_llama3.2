import argparse
import pandas as pd
from datetime import datetime
import torch
import torch.nn as nn
from torch.nn import functional as F
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    AutoConfig,
    TrainingArguments,
    Trainer,
    DataCollatorWithPadding,
    BitsAndBytesConfig,
    PreTrainedModel
)

from transformers.modeling_outputs import SequenceClassifierOutput
from transformers.models.llama.modeling_llama import LlamaModel
from datasets import load_dataset
import numpy as np
from sklearn.metrics import accuracy_score, f1_score, matthews_corrcoef
from scipy.stats import pearsonr, spearmanr
import math

# --- QST æ ¸å¿ƒæ¶æ„å®ç° ---

class AdapterModule(nn.Module):
    """
    è®ºæ–‡ä¸­æ¨èçš„ Downsampler å®ç° (Section 4.6, Table 6) [cite: 425]
    è¿™æ˜¯ä¸€ä¸ªç“¶é¢ˆ(bottleneck)ç»“æ„çš„é€‚é…å™¨ï¼šLinear -> Activation -> Linear
    """
    def __init__(self, in_features, out_features, bottleneck_dim, activation=nn.GELU()):
        super().__init__()
        self.down_proj = nn.Linear(in_features, bottleneck_dim)
        self.activation = activation
        self.up_proj = nn.Linear(bottleneck_dim, out_features)
        self.layer_norm = nn.LayerNorm(out_features)
        
    def forward(self, x):
        x_down = self.down_proj(x)
        x_act = self.activation(x_down)
        x_up = self.up_proj(x_act)
        return self.layer_norm(x_up)

class QSTLlamaForSequenceClassification(PreTrainedModel):
    """
    QST (Quantized Side Tuning) è®ºæ–‡æ¶æ„çš„å®Œæ•´å®ç°
    
    è¯¥æ¨¡å‹åŒ…å«:
    1. base_model (f): å†»ç»“çš„ 4-bit é‡åŒ– Llama æ¨¡å‹ [cite: 10]
    2. side_network (g): ä¸€ä¸ªå°å‹çš„ã€å¯è®­ç»ƒçš„ BF16 Llama æ¨¡å‹ [cite: 10]
    3. downsamplers: Nä¸ªé€‚é…å™¨æ¨¡å—ï¼Œå°† f çš„è¾“å‡ºç»´åº¦é™ä½åˆ° g çš„è¾“å…¥ç»´åº¦ [cite: 217]
    4. upsampler: 1ä¸ªçº¿æ€§å±‚ï¼Œå°† g çš„æœ€ç»ˆè¾“å‡ºæ¢å¤åˆ° f çš„ç»´åº¦ [cite: 224]
    5. gates (alpha, betas): å¯è®­ç»ƒçš„é—¨æ§å‚æ•° [cite: 213, 216]
    """
    config_class = AutoConfig # å‘Šè¯‰ Hugging Face è¿™æ˜¯ä¸€ä¸ª PreTrainedModel

    def __init__(self, config, base_model_4bit, reduction_factor_r=16, adapter_rank_r=16):
        super().__init__(config)
        
        # 1. ä¸»ç½‘ç»œ (f) - å†»ç»“çš„ 4-bit Llama 
        # self.base_model = base_model_4bit
        
        # 2. QST å‚æ•°
        self.num_layers = config.num_hidden_layers
        self.d_model = config.hidden_size
        self.d_side = self.d_model // reduction_factor_r 
        
        print(f"[QST] ä¸»ç½‘ç»œ (f) d_model: {self.d_model}")
        print(f"[QST] ä¾§ç½‘ç»œ (g) d_side: {self.d_side} (r={reduction_factor_r})")
        print(f"[QST] Downsampler ç§©: {adapter_rank_r}")

        # 3. ä¾§ç½‘ç»œ (g) - å¯è®­ç»ƒçš„ BF16 Llama [cite: 10]
        side_config = AutoConfig.from_pretrained(config._name_or_path, trust_remote_code=True)
        side_config.hidden_size = self.d_side
        side_config.num_hidden_layers = self.num_layers
        side_config.intermediate_size = side_config.intermediate_size // reduction_factor_r
        # æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªæ–°çš„LlamaModelä½œä¸ºä¾§ç½‘ç»œï¼Œä½†ä¸åŠ è½½å…¶é¢„è®­ç»ƒæƒé‡
        self.side_network = LlamaModel(side_config)
        
        # 4. Downsamplers (Nä¸ªå±‚ + 1ä¸ªåµŒå…¥å±‚)
        # è®ºæ–‡æåˆ°ä¹Ÿä¸‹é‡‡æ ·åµŒå…¥å±‚ [cite: 216]
        self.downsampler_embed = AdapterModule(self.d_model, self.d_side, adapter_rank_r)
        self.downsamplers_layers = nn.ModuleList(
            [AdapterModule(self.d_model, self.d_side, adapter_rank_r) for _ in range(self.num_layers)]
        )
        
        # 5. Upsampler [cite: 224]
        self.upsampler = nn.Linear(self.d_side, self.d_model)
        
        # 6. Gating å‚æ•° [cite: 213, 216]
        # betas: æ¯ä¸€å±‚çš„æ··åˆæƒé‡ï¼Œåˆå§‹åŒ–ä¸º0 [cite: 216]
        self.betas = nn.Parameter(torch.zeros(self.num_layers))
        # alpha: æœ€ç»ˆè¾“å‡ºçš„æ··åˆæƒé‡ï¼Œåˆå§‹åŒ–ä¸º1 [cite: 214]
        self.alpha = nn.Parameter(torch.tensor(1.0))
        
        # 7. åˆ†ç±»å¤´ (æˆ‘ä»¬å¤ç”¨ base_model çš„åˆ†ç±»å¤´)
        self.classifier = base_model_4bit.score

        # === å…³é”®ä¿®å¤ ===
        # åŒæ ·, æå– base_model çš„æ ¸å¿ƒ LlamaModel, 
        # ä»¥é¿å… __getattr__ å†²çª
        self.base_llama_model = base_model_4bit.model
        # === ä¿®å¤ç»“æŸ ===

        # 1. ä¸»ç½‘ç»œ (f) - ç°åœ¨å†èµ‹å€¼
        self.base_model = base_model_4bit
        
        # 8. å†»ç»“ä¸»ç½‘ç»œå¹¶è§£å†»QSTç»„ä»¶
        self.freeze_base_model_and_enable_qst()
        
        # 9. å…³é”®ä¿®å¤: ä¼ªè£…æˆPEFTæ¨¡å‹ç»•è¿‡Traineræ£€æŸ¥
        self._hf_peft_config_loaded = True
    
    def save_pretrained(self, save_directory, **kwargs):
        """è‡ªå®šä¹‰ä¿å­˜æ–¹æ³•ï¼Œåªä¿å­˜QSTä¾§ç½‘ç»œå‚æ•°"""
        import os, torch, json
        os.makedirs(save_directory, exist_ok=True)
        qst_state_dict = {name: param.cpu() for name, param in self.named_parameters() if param.requires_grad}
        torch.save(qst_state_dict, os.path.join(save_directory, "qst_adapter.bin"))
        json.dump({"model_type": "qst_llama", "num_labels": self.config.num_labels, "d_model": self.d_model, "d_side": self.d_side}, open(os.path.join(save_directory, "qst_config.json"), "w"), indent=2)
        print(f"âœ… QSTä¾§ç½‘ç»œå·²ä¿å­˜åˆ°: {save_directory} ({len(qst_state_dict)} å‚æ•°)")
        print("[QST] æ¨¡å‹åˆå§‹åŒ–å®Œæˆï¼Œä¸»ç½‘ç»œå·²å†»ç»“ã€‚")

    # ... åœ¨ __init__ æ–¹æ³•ç»“æŸå ...

    @staticmethod
    def _prepare_4d_causal_attention_mask(attention_mask, input_shape, dtype, device, past_key_values_length=0):
        """
        åœ¨æœ¬åœ°å¤ç° transformers å†…éƒ¨çš„æ©ç åˆ›å»ºé€»è¾‘
        """
        bsz, tgt_len = input_shape

        # [bsz, 1, tgt_len, tgt_len]
        # åˆ›å»ºä¸€ä¸ªå¡«å……äº†æå°å€¼ï¼ˆè¡¨ç¤º-infï¼‰çš„æ©ç 
        causal_mask = torch.full((bsz, 1, tgt_len, tgt_len), torch.finfo(dtype).min, dtype=dtype, device=device)
        
        # åˆ›å»ºå› æœï¼ˆcausalï¼‰éƒ¨åˆ†
        # æˆ‘ä»¬éœ€è¦ causal_mask[b, 0, i, j] = 0.0 å½“ j <= i æ—¶
        
        # 1. åˆ›å»ºä¸€ä¸ª [tgt_len] çš„å¼ é‡: [0, 1, 2, ..., tgt_len-1]
        mask_cond = torch.arange(tgt_len, device=device)
        
        # 2. åˆ›å»ºä¸€ä¸ª [tgt_len, tgt_len] çš„å¸ƒå°”æ©ç ï¼Œå…¶ä¸­ bool_mask[i, j] = (j <= i)
        #    è¿™æ˜¯é€šè¿‡å¹¿æ’­ (mask_cond < (mask_cond + 1).view(tgt_len, 1)) å®ç°çš„
        causal_bool_mask = mask_cond < (mask_cond + 1).view(tgt_len, 1)
        
        # 3. å°† [tgt_len, tgt_len] çš„å¸ƒå°”æ©ç åº”ç”¨åˆ° [bsz, 1, tgt_len, tgt_len] çš„ causal_mask
        #    å¸ƒå°”æ©ç ä¼šè‡ªåŠ¨å¹¿æ’­åˆ°æ­£ç¡®çš„ç»´åº¦
        causal_mask.masked_fill_(causal_bool_mask.bool(), 0.0)
        

        if past_key_values_length > 0:
            causal_mask[..., :, :past_key_values_length] = 0.0

        if attention_mask is not None:
            if attention_mask.dim() == 2:
                # [bsz, seq_len] -> [bsz, 1, 1, seq_len]
                attention_mask = attention_mask[:, None, None, :]

            # [bsz, 1, 1, seq_len] -> [bsz, 1, tgt_len, tgt_len]
            attention_mask = attention_mask.expand((bsz, 1, tgt_len, tgt_len))
            # å°† padding æ©ç  (attention_mask == 0) åº”ç”¨åˆ°å› æœæ©ç 
            causal_mask = causal_mask.masked_fill(attention_mask == 0, torch.finfo(dtype).min)

        return causal_mask

    # ... get_input_embeddings æ–¹æ³•å¼€å§‹çš„åœ°æ–¹ ...

    def freeze_base_model_and_enable_qst(self):
        # å†»ç»“æ‰€æœ‰ base_model å‚æ•° [cite: 229]
        self.base_model.requires_grad_(False)
        # ç¡®ä¿ QST ç»„ä»¶æ˜¯å¯è®­ç»ƒçš„ (å®ƒä»¬é»˜è®¤æ˜¯)
        self.side_network.requires_grad_(True)
        self.downsampler_embed.requires_grad_(True)
        self.downsamplers_layers.requires_grad_(True)
        self.upsampler.requires_grad_(True)
        self.betas.requires_grad_(True)
        self.alpha.requires_grad_(True)
        # è§£å†»æˆ‘ä»¬å¤ç”¨çš„åˆ†ç±»å¤´
        self.classifier.requires_grad_(True)
        
    def get_input_embeddings(self):
        return self.base_model.model.embed_tokens

    def set_input_embeddings(self, value):
        self.base_model.model.embed_tokens = value

    def forward(
        self,
        input_ids,
        attention_mask,
        labels=None,  # <-- 1. æ¢å¤ labels=None
        **kwargs  
    ):
        # 0. å‡†å¤‡ä¾§ç½‘ç»œçš„æ³¨æ„åŠ›æ©ç 
        batch_size, seq_length = input_ids.shape
        position_ids = torch.arange(0, seq_length, dtype=torch.long, device=input_ids.device)
        position_ids = position_ids.unsqueeze(0).view(-1, seq_length)

        side_attention_mask = self._prepare_4d_causal_attention_mask(
            attention_mask, 
            (batch_size, seq_length), 
            past_key_values_length=0, 
            dtype=self.side_network.dtype,
            device=input_ids.device
        )

        # 1. è¿è¡Œä¸»ç½‘ç»œ (f) - æ— æ¢¯åº¦
        with torch.no_grad():
            base_outputs = self.base_llama_model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                position_ids=position_ids,
                output_hidden_states=True
            )
            base_hidden_states = base_outputs.hidden_states

        # 2. è¿è¡Œä¾§ç½‘ç»œ (g) - æœ‰æ¢¯åº¦
        h_f_0 = base_hidden_states[0]
        h_g_prev = self.downsampler_embed(h_f_0)
        
        side_position_embeddings = self.side_network.rotary_emb(h_g_prev, position_ids)
        
        for i in range(self.num_layers):
            h_f_i = base_hidden_states[i + 1]
            downsampled_h_f_i = self.downsamplers_layers[i](h_f_i)
            beta_i = torch.sigmoid(self.betas[i])
            side_input = (1 - beta_i) * downsampled_h_f_i + beta_i * h_g_prev
            
            layer_outputs = self.side_network.layers[i](
                side_input,
                attention_mask=side_attention_mask,
                position_embeddings=side_position_embeddings,
            )
            h_g_prev = layer_outputs[0]

        h_g_N = h_g_prev
        h_f_N = base_hidden_states[-1]
        
        final_hidden_state = self.alpha * h_f_N + (1 - self.alpha) * self.upsampler(h_g_N)

        # 4. åˆ†ç±»
        batch_size = input_ids.shape[0]
        if self.config.pad_token_id is None:
             sequence_lengths = -1
        else:
            sequence_lengths = (input_ids != self.config.pad_token_id).sum(-1) - 1
            
        last_token_hidden_states = final_hidden_state[torch.arange(batch_size, device=final_hidden_state.device), sequence_lengths]
        
        logits = self.classifier(last_token_hidden_states)

        # 5. è®¡ç®—æŸå¤±
        loss = None
        final_labels = labels if labels is not None else kwargs.get("labels")
        
        if final_labels is not None:
            if self.config.num_labels == 1:
                loss_fct = nn.MSELoss()
                loss = loss_fct(logits.squeeze(), final_labels.squeeze())
            elif self.config.num_labels > 1:
                loss_fct = nn.CrossEntropyLoss()
                loss = loss_fct(logits.view(-1, self.config.num_labels), final_labels.view(-1))

        return SequenceClassifierOutput(
            loss=loss,
            logits=logits,
            hidden_states=None, # æˆ‘ä»¬ä¸è¿”å›éšè—çŠ¶æ€ä»¥èŠ‚çœå†…å­˜
            attentions=None,
        )


# --- è®­ç»ƒè„šæœ¬ (ä¸æ‚¨çš„åŸä»£ç ç±»ä¼¼) ---

DEFAULT_PAD_TOKEN = "[PAD]"

task_to_keys = {
    "cola": ("sentence", None),
    "mnli": ("premise", "hypothesis"),
    "mnli-mm": ("premise", "hypothesis"),
    "mrpc": ("sentence1", "sentence2"),
    "qnli": ("question", "sentence"),
    "qqp": ("question1", "question2"),
    "rte": ("sentence1", "sentence2"),
    "sst2": ("sentence", None),
    "stsb": ("sentence1", "sentence2"),
    "wnli": ("sentence1", "sentence2"),
}

def compute_metrics_sklearn(task, eval_pred):
    predictions, labels = eval_pred
    if task == "stsb":
        predictions = predictions[:, 0]
        pearson_corr = pearsonr(predictions, labels)[0]
        spearman_corr = spearmanr(predictions, labels)[0]
        return {
            "pearson": pearson_corr,
            "spearmanr": spearman_corr,
            "combined": (pearson_corr + spearman_corr) / 2
        }
    else:
        predictions = np.argmax(predictions, axis=1)
        acc = accuracy_score(labels, predictions)
        f1 = f1_score(labels, predictions, average='macro')
        if task == "cola":
            mcc = matthews_corrcoef(labels, predictions)
            return {"matthews_correlation": mcc, "accuracy": acc, "f1": f1}
        elif task in ["mrpc", "qqp"]:
            return {"accuracy": acc, "f1": f1}
        else:
            return {"accuracy": acc, "f1": f1}

def print_trainable_parameters(model):
    trainable_params = 0
    all_param = 0
    for _, param in model.named_parameters():
        all_param += param.numel()
        if param.requires_grad:
            trainable_params += param.numel()
    print(
        f"\nğŸ“Š å‚æ•°ç»Ÿè®¡:"
        f"\n Â å¯è®­ç»ƒå‚æ•°: {trainable_params:,}"
        f"\n Â æ€»å‚æ•°: {all_param:,}"
        f"\n Â å¯è®­ç»ƒæ¯”ä¾‹: {100 * trainable_params / all_param:.4f}%"
    )

def train_qst_model(task, parameters):
    model_checkpoint = parameters["model_checkpoint"]
    batch_size = parameters["batch_size"]
    max_len = parameters["max_len"]
    epochs = parameters["epochs"]
    r = parameters.get("r", 16) # è®ºæ–‡é»˜è®¤r=16 [cite: 253]
    alpha_r = parameters.get("alpha_r", 16) # è®ºæ–‡ä¸­Downsamplerçš„ç§© [cite: 254]

    print("\n" + "="*60)
    print(f"QST (è®ºæ–‡å®ç°) 4-bité‡åŒ–è®­ç»ƒ: {task}")
    print(f"æ¨¡å‹: {model_checkpoint}, ä¾§ç½‘ç»œr: {r}, Downsamplerç§©: {alpha_r}")
    print("="*60 + "\n")
    
    actual_task = "mnli" if task == "mnli-mm" else task
    dataset = load_dataset("nyu-mll/glue", actual_task)
    
    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True, trust_remote_code=True)
    num_labels = 3 if task.startswith("mnli") else 1 if task == "stsb" else 2
    
    # 1. 4-bit é‡åŒ–é…ç½® [cite: 9, 76]
    quant_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4", # è®ºæ–‡æ¨è NF4 [cite: 167, 254, 415]
        bnb_4bit_compute_dtype=torch.bfloat16
    )
    
    # å°†æ¨¡å‹å®Œå…¨åŠ è½½åˆ°å•ä¸ªGPUä¸Š
    compute_device = "cuda:0" if torch.cuda.is_available() else "cpu"
    
    print(f"åŠ è½½4-bité‡åŒ–ä¸»ç½‘ç»œ (f) åˆ° {compute_device}: {model_checkpoint}")
    base_model_4bit = AutoModelForSequenceClassification.from_pretrained(
        model_checkpoint,
        quantization_config=quant_config,
        torch_dtype=torch.bfloat16,
        num_labels=num_labels,
        device_map=compute_device, # å°†æ•´ä¸ªæ¨¡å‹æ”¾åœ¨ä¸€ä¸ªè®¾å¤‡ä¸Š
        trust_remote_code=True,
        attn_implementation="eager"
    )
    
    if tokenizer.pad_token is None:
        tokenizer.add_special_tokens({'pad_token': DEFAULT_PAD_TOKEN})
        base_model_4bit.resize_token_embeddings(len(tokenizer))
    
    base_model_4bit.config.pad_token_id = tokenizer.pad_token_id
    
    # 2. åˆ›å»º QST åŒ…è£…æ¨¡å‹
    print("åˆ›å»º QST åŒ…è£…æ¨¡å‹ (f + g)...")
    model = QSTLlamaForSequenceClassification(
        config=base_model_4bit.config,
        base_model_4bit=base_model_4bit,
        reduction_factor_r=r,
        adapter_rank_r=alpha_r
    )
    # å°†æ–°åˆ›å»ºçš„ QST ç»„ä»¶ (ä¾§ç½‘ç»œç­‰) ç§»åŠ¨åˆ° GPU
    model.to(compute_device, dtype=torch.bfloat16)

    print_trainable_parameters(model)
    
    # 3. æ•°æ®é¢„å¤„ç†
    sentence1_key, sentence2_key = task_to_keys[task]
    
    def preprocess_function(examples):
        if sentence2_key is None:
            return tokenizer(examples[sentence1_key], truncation=True, padding='max_length', max_length=max_len)
        return tokenizer(examples[sentence1_key], examples[sentence2_key], truncation=True, padding='max_length', max_length=max_len)
    
    print("æ•°æ®é¢„å¤„ç†...")
    encoded_dataset = dataset.map(preprocess_function, batched=True)
    
    validation_key = "validation_mismatched" if task == "mnli-mm" else "validation_matched" if task == "mnli" else "validation"
    
    # 4. è®­ç»ƒ
    metric_name = "pearson" if task == "stsb" else "matthews_correlation" if task == "cola" else "accuracy"
    args = TrainingArguments(
        f"llama3-qst-4bit-{task}",
        eval_strategy="epoch",
        save_strategy="epoch",
        learning_rate=2e-4, # è®ºæ–‡åœ¨ MMLU ä¸Šä½¿ç”¨ 2E-04 [cite: 671, 681]
        per_device_train_batch_size=batch_size,
        per_device_eval_batch_size=batch_size,
        num_train_epochs=epochs,
        weight_decay=0.01,
        load_best_model_at_end=True,
        metric_for_best_model=metric_name,
        push_to_hub=False,
        fp16=False,
        bf16=True, # å¿…é¡»ä½¿ç”¨ BF16 [cite: 254]
        logging_steps=100,
        save_total_limit=2,
        report_to="none",
    )
    
    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
    
    trainer = Trainer(
        model,
        args,
        train_dataset=encoded_dataset["train"],
        eval_dataset=encoded_dataset[validation_key],
        tokenizer=tokenizer,
        data_collator=data_collator,
        compute_metrics=lambda x: compute_metrics_sklearn(task, x)
    )
    
    print("ğŸš€ å¼€å§‹ QST è®­ç»ƒ...")
    if torch.cuda.is_available():
        torch.cuda.reset_peak_memory_stats()
    trainer.train()
    peak_memory_gb = 0
    if torch.cuda.is_available():
        peak_memory_gb = torch.cuda.max_memory_allocated() / (1024**3)
    
    print("\nğŸ“ˆ è¯„ä¼°æœ€ç»ˆæ¨¡å‹...")
    final_metrics = trainer.evaluate()
    final_metrics["peak_memory_gb"] = peak_memory_gb
    final_metrics["trainable_params"] = sum(p.numel() for p in model.parameters() if p.requires_grad)
    final_metrics["total_params"] = sum(p.numel() for p in model.parameters())
    final_metrics["trainable_ratio"] = (final_metrics["trainable_params"] / final_metrics["total_params"]) * 100
    return final_metrics

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="QST (è®ºæ–‡å®ç°) + 4-bité‡åŒ–è®­ç»ƒ")
    parser.add_argument("--model_checkpoint", type=str, default="meta-llama/Llama-3.2-1B")
    parser.add_argument("--batch_size", type=int, default=8) # 1B æ¨¡å‹å¯ä»¥å°è¯•ç¨å¤§çš„æ‰¹é‡
    parser.add_argument("--max_len", type=int, default=512)
    parser.add_argument("--epochs", type=int, default=3)
    parser.add_argument("--task", type=str, default="sst2", help=f"GLUE ä»»åŠ¡: {list(task_to_keys.keys())}")
    parser.add_argument("--r", type=int, default=16, help="ä¾§ç½‘ç»œç¼©å‡å› å­ (è®ºæ–‡é»˜è®¤16)") # [cite: 253]
    parser.add_argument("--alpha_r", type=int, default=16, help="Downsampler é€‚é…å™¨ç§© (è®ºæ–‡é»˜è®¤16)") # [cite: 254]
    
    args = parser.parse_args()
    
    parameters = {
        "model_checkpoint": args.model_checkpoint,
        "batch_size": args.batch_size,
        "max_len": args.max_len,
        "epochs": args.epochs,
        "r": args.r,
        "alpha_r": args.alpha_r,
    }
    
    tasks = [args.task]
    
    results = {}
    for task in tasks:
        try:
            results[task] = train_qst_model(task, parameters)
        except Exception as e:
            print(f"\nâŒ ä»»åŠ¡ {task} è®­ç»ƒå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            continue
            
    print("\n" + "="*60)
    print("è®­ç»ƒå®Œæˆ! ç»“æœ:")
    print("="*60)
    for task, result in results.items():
        print(f"\n{task}:")
        for metric, value in result.items():
            print(f" Â {metric}: {value:.4f}")
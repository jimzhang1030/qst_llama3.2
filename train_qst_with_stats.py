from datetime import datetime
import os, torch, random, argparse
import numpy as np
from torch import nn
from torch.nn import functional as F
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    AutoConfig,
    TrainingArguments,
    Trainer,
    DataCollatorWithPadding,
    BitsAndBytesConfig,
    PreTrainedModel
)

from transformers.modeling_outputs import SequenceClassifierOutput
from transformers.models.llama.modeling_llama import LlamaModel
from datasets import load_dataset
import json
from sklearn.metrics import accuracy_score, f1_score, matthews_corrcoef
from scipy.stats import pearsonr, spearmanr
import pandas as pd

# ä»»åŠ¡ç‰¹å®šè¶…å‚æ•°ï¼ˆä¼˜åŒ–ç‰ˆ - ä¿®å¤å‡†ç¡®ç‡ä¸‹é™é—®é¢˜ï¼‰

TASK_HYPERPARAMS = {
    # å°æ•°æ®é›†ä»»åŠ¡ï¼šæ›´å°å­¦ä¹ ç‡ï¼Œæ›´å¤šepoch
    "rte": {"epochs": 20, "batch_size": 8, "lr": 3e-4, "warmup_ratio": 0.06, "max_len": 256},
    "mrpc": {"epochs": 20, "batch_size": 8, "lr": 3e-4, "warmup_ratio": 0.06, "max_len": 256},
    "stsb": {"epochs": 20, "batch_size": 8, "lr": 3e-4, "warmup_ratio": 0.06, "max_len": 256},
    "cola": {"epochs": 20, "batch_size": 8, "lr": 3e-4, "warmup_ratio": 0.1, "max_len": 256},
    # å¤§æ•°æ®é›†ä»»åŠ¡ï¼šç¨å¤§å­¦ä¹ ç‡
    "sst2": {"epochs": 5, "batch_size": 16, "lr": 3e-4, "warmup_ratio": 0.06, "max_len": 128},
    "qnli": {"epochs": 5, "batch_size": 8, "lr": 3e-4, "warmup_ratio": 0.06, "max_len": 256},
    "qqp": {"epochs": 5, "batch_size": 8, "lr": 3e-4, "warmup_ratio": 0.06, "max_len": 256},
    "mnli": {"epochs": 5, "batch_size": 16, "lr": 3e-4, "warmup_ratio": 0.06, "max_len": 256},
}

# --- QST æ ¸å¿ƒæ¶æ„å®ç° (å®Œå…¨é€‚é…æœ€æ–°transformers + Llama3.2) ---
class QSTTrainer(Trainer):
    def _save(self, output_dir=None, state_dict=None):
        """è¦†ç›–ä¿å­˜æ–¹æ³•ä»¥é¿å… PEFT é›†æˆé—®é¢˜"""
        output_dir = output_dir if output_dir is not None else self.args.output_dir
        os.makedirs(output_dir, exist_ok=True)
        
        # è·å–å®é™…æ¨¡å‹ï¼ˆå¤„ç† DataParallel åŒ…è£…ï¼‰
        model = self.model.module if hasattr(self.model, 'module') else self.model
        
        # åªä¿å­˜å¯è®­ç»ƒçš„ QST ç»„ä»¶
        qst_state_dict = {
            'qst_layers': model.qst_layers.state_dict(),
            'downsamplers_layers': model.downsamplers_layers.state_dict(),
            'downsampler_embed': model.downsampler_embed.state_dict(),
            'z': model.z,
            'score_z': model.score_z,
            'norm_qst': model.norm_qst.state_dict(),
            'upsampler': model.upsampler.state_dict(),
            'classifier': model.classifier.state_dict(),
        }
        
        # ä¿å­˜ QST ç»„ä»¶ï¼ˆè‡ªå®šä¹‰æ ¼å¼ï¼‰
        torch.save(qst_state_dict, os.path.join(output_dir, 'qst_components.bin'))
        
        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šåŒæ—¶ä¿å­˜ä¸º pytorch_model.binï¼Œè®© load_best_model_at_end èƒ½æ‰¾åˆ°
        torch.save(qst_state_dict, os.path.join(output_dir, 'pytorch_model.bin'))
        
        # ä¿å­˜è®­ç»ƒå‚æ•°
        torch.save(self.args, os.path.join(output_dir, 'training_args.bin'))
        
        # ğŸ”¥ æ–°å¢ï¼šä¿å­˜ trainer_state.jsonï¼ˆTrainer ç”¨æ¥è¿½è¸ªæœ€ä½³æ¨¡å‹ï¼‰
        # TrainerState æœ¬èº«å°±æ˜¯å¯åºåˆ—åŒ–çš„ï¼Œä¸éœ€è¦ to_dict()
        if hasattr(self, 'state'):
            import json
            # TrainerState å¯ä»¥ç›´æ¥ä½¿ç”¨ asdict è½¬æ¢
            from dataclasses import asdict
            with open(os.path.join(output_dir, 'trainer_state.json'), 'w') as f:
                json.dump(asdict(self.state), f, indent=2)
        
        print(f"âœ… QST ç»„ä»¶å·²ä¿å­˜åˆ°: {output_dir}")

    def _save_checkpoint(self, model, trial, metrics=None):
        """
        è¦†ç›– _save_checkpoint ä»¥ç¡®ä¿ load_best_model_at_end æ­£å¸¸å·¥ä½œ
        """
        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šè°ƒç”¨çˆ¶ç±»çš„ _save_checkpointï¼Œå®ƒä¼šæ­£ç¡®å¤„ç†æ‰€æœ‰é€»è¾‘
        # åŒ…æ‹¬ï¼šä¿å­˜æ¨¡å‹ã€æ›´æ–° best_model_checkpointã€æ¸…ç†æ—§ checkpoint
        checkpoint_folder = super()._save_checkpoint(model, trial)
        return checkpoint_folder

    def _load_best_model(self):
        """
        é‡å†™åŠ è½½æœ€ä½³æ¨¡å‹çš„æ–¹æ³•ï¼Œä½¿ç”¨æˆ‘ä»¬è‡ªå·±çš„ QST checkpoint æ ¼å¼
        """
        import os
        
        # è·å–æœ€ä½³ checkpoint è·¯å¾„
        best_model_checkpoint = self.state.best_model_checkpoint
        
        if best_model_checkpoint is None:
            print("âš ï¸ æœªæ‰¾åˆ°æœ€ä½³æ¨¡å‹ checkpointï¼Œä½¿ç”¨å½“å‰æ¨¡å‹")
            return
        
        print(f"\nğŸ”„ åŠ è½½æœ€ä½³æ¨¡å‹: {best_model_checkpoint}")
        
        # ä½¿ç”¨æˆ‘ä»¬è‡ªå·±çš„åŠ è½½æ–¹æ³•
        self._load_from_checkpoint(best_model_checkpoint)
        
        print(f"âœ… å·²æˆåŠŸåŠ è½½æœ€ä½³æ¨¡å‹ (å‡†ç¡®ç‡: {self.state.best_metric:.4f})")

    def _load_from_checkpoint(self, resume_from_checkpoint):
        """è¦†ç›–åŠ è½½æ–¹æ³•ä»¥æ”¯æŒ QST checkpoint æ ¼å¼"""
        import os
        import torch
        
        # å°è¯•åŠ è½½ pytorch_model.binï¼ˆæˆ‘ä»¬æ–°ä¿å­˜çš„æ ¼å¼ï¼‰
        checkpoint_path = os.path.join(resume_from_checkpoint, 'pytorch_model.bin')
        
        if not os.path.exists(checkpoint_path):
            # é™çº§åˆ° qst_components.bin
            checkpoint_path = os.path.join(resume_from_checkpoint, 'qst_components.bin')
        
        if os.path.exists(checkpoint_path):
            print(f"ğŸ”„ ä» {checkpoint_path} åŠ è½½ QST checkpoint...")
            qst_state_dict = torch.load(checkpoint_path, map_location=self.model.device)
            
            # è·å–å®é™…æ¨¡å‹
            model = self.model.module if hasattr(self.model, 'module') else self.model
            
            # åŠ è½½å„ä¸ªç»„ä»¶
            model.qst_layers.load_state_dict(qst_state_dict['qst_layers'])
            model.downsamplers_layers.load_state_dict(qst_state_dict['downsamplers_layers'])
            model.downsampler_embed.load_state_dict(qst_state_dict['downsampler_embed'])
            model.z.data = qst_state_dict['z']
            model.score_z.data = qst_state_dict['score_z']
            model.norm_qst.load_state_dict(qst_state_dict['norm_qst'])
            model.upsampler.load_state_dict(qst_state_dict['upsampler'])
            model.classifier.load_state_dict(qst_state_dict['classifier'])
            
            print(f"âœ… æˆåŠŸåŠ è½½ QST checkpoint")
        else:
            print(f"âš ï¸ æœªæ‰¾åˆ° checkpoint æ–‡ä»¶: {checkpoint_path}")

class AdapterModule(nn.Module):
    """è®ºæ–‡æ¨èçš„Downsamplerå®ç°"""
    def __init__(self, in_features, out_features, bottleneck_dim, activation=nn.SiLU()):
        super().__init__()
        self.down_proj = nn.Linear(in_features, bottleneck_dim, bias=False)
        self.activation = activation
        self.up_proj = nn.Linear(bottleneck_dim, out_features, bias=False)
        self.dropout = nn.Dropout(p=0.1)
        nn.init.kaiming_uniform_(self.down_proj.weight, a=0, mode="fan_in", nonlinearity="linear")
        nn.init.kaiming_uniform_(self.up_proj.weight, a=0, mode="fan_in", nonlinearity="linear")
        self.layer_norm = nn.LayerNorm(out_features)
        
    
    def forward(self, x):
        x = self.dropout(x)
        x_down = self.down_proj(x)
        x_act = self.activation(x_down)
        x_up = self.up_proj(x_act)
        return self.layer_norm(x_up)

class QSTLlamaForSequenceClassification(PreTrainedModel):
    """
    QST (Quantized Side Tuning) - å®Œå…¨é€‚é…æœ€æ–°transformers + Llama3.2
    
    å…³é”®ä¿®å¤:
    1. ä½¿ç”¨backbone.forward()è€Œä¸æ˜¯ç›´æ¥è°ƒç”¨layer (é¿å…position_embeddings/cache_positioné—®é¢˜)
    2. output_hidden_states=Trueè·å–æ‰€æœ‰å±‚è¾“å‡º
    3. ä¾§ç½‘ç»œlayersæ˜¯æ–°åˆ›å»ºçš„ï¼Œå¯ä»¥ç›´æ¥è°ƒç”¨
    """
    config_class = AutoConfig

    def __init__(self, config, base_model_4bit, reduction_factor_r=16, adapter_rank_r=16):
        super().__init__(config)
        
        self.num_layers = config.num_hidden_layers
        self.d_model = config.hidden_size
        self.d_side = self.d_model // reduction_factor_r 
        self.num_labels = config.num_labels
        
        print(f"[QST] ä¸»ç½‘ç»œ (f) d_model: {self.d_model}")
        print(f"[QST] ä¾§ç½‘ç»œ (g) d_side: {self.d_side} (r={reduction_factor_r})")
        print(f"[QST] Downsampler ç§©: {adapter_rank_r}")

        # ä¾§ç½‘ç»œé…ç½®
        side_config = AutoConfig.from_pretrained(config._name_or_path, trust_remote_code=True)
        side_config.hidden_size = self.d_side
        side_config.num_hidden_layers = self.num_layers
        side_config.intermediate_size = side_config.intermediate_size // reduction_factor_r
        side_num_heads = max(1, self.d_side // 64)
        side_config.num_attention_heads = side_num_heads
        side_config.num_key_value_heads = side_num_heads
        
        # åªä¿å­˜ä¾§ç½‘ç»œçš„layerså’Œnorm
        side_network = LlamaModel(side_config)
        self.qst_layers = side_network.layers
        self.norm_qst = side_network.norm
        del side_network
        
        # Downsamplers
        self.downsampler_embed = AdapterModule(self.d_model, self.d_side, adapter_rank_r)
        self.downsamplers_layers = nn.ModuleList(
            [AdapterModule(self.d_model, self.d_side, adapter_rank_r) for _ in range(self.num_layers)]
        )
        
        # Upsampler
        self.upsampler = nn.Linear(self.d_side, self.d_model)
        
        # é—¨æ§å‚æ•°
        self.z = nn.Parameter(torch.zeros(self.num_layers))
        self.score_z = nn.Parameter(torch.ones(self.d_model))
        
        # åˆ†ç±»å¤´ & backbone
        self.classifier = base_model_4bit.score
        self.backbone = base_model_4bit.model
        self.base_model = base_model_4bit
        
        # å†»ç»“ä¸»ç½‘ç»œ
        self.freeze_base_model_and_enable_qst()
        
        # ä¼ªè£…æˆPEFTæ¨¡å‹
        self._hf_peft_config_loaded = True
    
    def freeze_base_model_and_enable_qst(self):
        for param in self.backbone.parameters():
            param.requires_grad = False
        for param in self.qst_layers.parameters():
            param.requires_grad = True
        for param in self.norm_qst.parameters():
            param.requires_grad = True
        for param in self.downsampler_embed.parameters():
            param.requires_grad = True
        for param in self.downsamplers_layers.parameters():
            param.requires_grad = True
        for param in self.upsampler.parameters():
            param.requires_grad = True
        self.z.requires_grad = True
        self.score_z.requires_grad = True
        for param in self.classifier.parameters():
            param.requires_grad = True
        
    def get_input_embeddings(self):
        return self.backbone.embed_tokens

    def set_input_embeddings(self, value):
        self.backbone.embed_tokens = value

    def get_adapter_state_dict(self, *args, **kwargs):
        """Override to avoid PEFT integration bug - we don't use PEFT"""
        return {}
    
    
    def forward(self, input_ids, attention_mask=None, labels=None):
        """
        å‰å‘ä¼ æ’­ - å®Œå…¨é€‚é…æœ€æ–°transformers + Llama3.2
        
        æ ¸å¿ƒç­–ç•¥: ä½¿ç”¨backbone.forward()è€Œä¸æ˜¯æ‰‹åŠ¨è°ƒç”¨æ¯ä¸€å±‚
        è¿™æ ·transformersä¼šè‡ªåŠ¨å¤„ç†æ‰€æœ‰å…¼å®¹æ€§é—®é¢˜
        """
        batch_size, seq_length = input_ids.shape
        device = input_ids.device
        
        # Position IDs
        if attention_mask is not None:
            position_ids = attention_mask.long().cumsum(-1) - 1
            position_ids.masked_fill_(attention_mask == 0, 1)
        else:
            position_ids = torch.arange(seq_length, dtype=torch.long, device=device)
            position_ids = position_ids.unsqueeze(0).expand(batch_size, -1)
        
        # ===== å…³é”®ä¿®å¤: ä½¿ç”¨LlamaModel.forward() =====
        # è¿™ä¼šè‡ªåŠ¨å¤„ç†cache_position, position_embeddingsç­‰æ‰€æœ‰å…¼å®¹æ€§é—®é¢˜
        with torch.no_grad():
            backbone_outputs = self.backbone(
                input_ids=input_ids,
                attention_mask=attention_mask,
                position_ids=position_ids,
                output_hidden_states=True,  # è·å–æ‰€æœ‰å±‚çš„hidden_states
                return_dict=True,
            )
            
            hidden_states = backbone_outputs.last_hidden_state
            all_backbone_hidden_states = backbone_outputs.hidden_states  # tuple: (embed, layer1, ..., layerN)
        
        # ä¾§ç½‘ç»œè¿è¡Œ
        qst_hidden_states = self.downsampler_embed(all_backbone_hidden_states[0])  # embedding layer
        
        # ä¸ºä¾§ç½‘ç»œåˆ›å»ºposition_embeddings (ä¾§ç½‘ç»œä¹Ÿéœ€è¦RoPE)
        from transformers.models.llama.modeling_llama import LlamaRotaryEmbedding
        if not hasattr(self, '_qst_rotary_emb'):
            # æ‡’åˆå§‹åŒ–ï¼šæœ€æ–°ç‰ˆ transformers åªæ¥å— config å‚æ•°
            config_copy = self.qst_layers[0].self_attn.config
            self._qst_rotary_emb = LlamaRotaryEmbedding(config=config_copy, device=qst_hidden_states.device)
        
        # è®¡ç®—ä¾§ç½‘ç»œçš„position_embeddings
        qst_position_embeddings = self._qst_rotary_emb(qst_hidden_states, position_ids)
        
        # ä¸º QST å±‚å‡†å¤‡ 4D causal attention_mask å’Œ cache_position
        # ä½¿ç”¨ transformers å†…ç½®æ–¹æ³•ç¡®ä¿æ ¼å¼æ­£ç¡®
        from transformers.modeling_attn_mask_utils import AttentionMaskConverter
        
        if attention_mask is not None:
            # åˆ›å»º 4D causal attention mask (ä¸ LlamaModel å†…éƒ¨é€»è¾‘ä¸€è‡´)
            attention_mask_converter = AttentionMaskConverter(is_causal=True, sliding_window=None)
            # to_4d è¿”å›çš„ mask å·²ç»åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
            qst_attention_mask = attention_mask_converter.to_4d(
                attention_mask,
                query_length=qst_hidden_states.shape[1],
                key_value_length=qst_hidden_states.shape[1],  # causal mask éœ€è¦
                dtype=qst_hidden_states.dtype,
            )
        else:
            qst_attention_mask = None
        
        # åˆ›å»º cache_position (QST å±‚éœ€è¦)
        cache_position = torch.arange(qst_hidden_states.shape[1], device=qst_hidden_states.device)
        
        for idx in range(self.num_layers):
            # Zé—¨æ§
            z_gate = torch.sigmoid(self.z[idx])
            downsampled = self.downsamplers_layers[idx](all_backbone_hidden_states[idx + 1])
            qst_hidden_states = (1 - z_gate) * downsampled + z_gate * qst_hidden_states
            
            # ä¾§ç½‘ç»œlayer (éœ€è¦ä¼ å…¥position_embeddingså’Œcache_position)
            layer_outputs = self.qst_layers[idx](
                qst_hidden_states,
                attention_mask=qst_attention_mask,  # 4D causal mask
                position_ids=position_ids,
                cache_position=cache_position,  # æœ€æ–° transformers å¿…éœ€
                position_embeddings=qst_position_embeddings,  # RoPE embeddings
            )
            qst_hidden_states = layer_outputs[0]
        
        qst_hidden_states = self.norm_qst(qst_hidden_states)
        
        # ç»†ç²’åº¦æ··åˆ
        score_z_gate = torch.sigmoid(self.score_z)
        upsampled_side = self.upsampler(qst_hidden_states)
        final_hidden = (1 - score_z_gate) * upsampled_side + score_z_gate * hidden_states
        
        # åˆ†ç±»ï¼šä½¿ç”¨æœ€åä¸€ä¸ªæœ‰æ•ˆtokenï¼ˆLlamaæ˜¯å› æœæ¨¡å‹ï¼Œä¸æ˜¯BERTï¼‰
        # è¿™æ˜¯å…³é”®ä¿®å¤ï¼šåŸä½œè€…ç”¨æœ€åä¸€ä¸ªtokenï¼Œä½ ç”¨çš„æ˜¯ç¬¬ä¸€ä¸ªtokenï¼
        batch_size = input_ids.shape[0]
        if self.config.pad_token_id is None:
            sequence_lengths = -1
        else:
            # æ‰¾åˆ°æ¯ä¸ªæ ·æœ¬çš„æœ€åä¸€ä¸ªépadding token
            sequence_lengths = (torch.eq(input_ids, self.config.pad_token_id).long().argmax(-1) - 1).to(final_hidden.device)
            # å¦‚æœæ•´è¡Œéƒ½ä¸æ˜¯paddingï¼Œargmaxè¿”å›0ï¼Œæ­¤æ—¶åº”è¯¥ç”¨æœ€åä¸€ä¸ªtoken
            sequence_lengths = torch.where(
                (attention_mask.sum(dim=1) == attention_mask.shape[1]),  # æ²¡æœ‰padding
                torch.tensor(input_ids.shape[1] - 1, device=final_hidden.device),
                sequence_lengths
            )
        
        pooled_hidden = final_hidden[torch.arange(batch_size, device=final_hidden.device), sequence_lengths]
        logits = self.classifier(pooled_hidden)
        
        # æŸå¤±
        loss = None
        if labels is not None:
            if self.num_labels == 1:
                loss_fct = nn.MSELoss()
                loss = loss_fct(logits.squeeze(), labels.squeeze())
            else:
                loss_fct = nn.CrossEntropyLoss()
                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))
        
        return {"loss": loss, "logits": logits}

# --- è®­ç»ƒè„šæœ¬ ---

DEFAULT_PAD_TOKEN = "[PAD]"

task_to_keys = {
    "cola": ("sentence", None),
    "mnli": ("premise", "hypothesis"),
    "mnli-mm": ("premise", "hypothesis"),
    "mrpc": ("sentence1", "sentence2"),
    "qnli": ("question", "sentence"),
    "qqp": ("question1", "question2"),
    "rte": ("sentence1", "sentence2"),
    "sst2": ("sentence", None),
    "stsb": ("sentence1", "sentence2"),
    "wnli": ("sentence1", "sentence2"),
}

def compute_metrics_sklearn(task, eval_pred):
    predictions, labels = eval_pred
    if task == "stsb":
        predictions = predictions.squeeze()
        labels = labels.squeeze()
        pearson_corr = pearsonr(predictions, labels)[0]
        spearman_corr = spearmanr(predictions, labels)[0]
        return {
            "pearson": pearson_corr,
            "spearmanr": spearman_corr,
            "corr": (pearson_corr + spearman_corr) / 2,
        }
    else:
        predictions = np.argmax(predictions, axis=1)
        accuracy = accuracy_score(labels, predictions)
        if task == "cola":
            mcc = matthews_corrcoef(labels, predictions)
            return {"accuracy": accuracy, "matthews_correlation": mcc}
        elif task in ["mrpc", "qqp"]:
            f1 = f1_score(labels, predictions)
            return {"accuracy": accuracy, "f1": f1}
        else:
            return {"accuracy": accuracy}

def print_trainable_parameters(model):
    trainable_params = 0
    all_param = 0
    for _, param in model.named_parameters():
        all_param += param.numel()
        if param.requires_grad:
            trainable_params += param.numel()
    print(
        f"\nğŸ“Š å‚æ•°ç»Ÿè®¡:"
        f"\n  å¯è®­ç»ƒå‚æ•°: {trainable_params:,}"
        f"\n  æ€»å‚æ•°: {all_param:,}"
        f"\n  å¯è®­ç»ƒæ¯”ä¾‹: {100 * trainable_params / all_param:.4f}%"
    )

def train_qst_model(task, parameters):
    model_checkpoint = parameters["model_checkpoint"]
    batch_size = parameters["batch_size"]
    max_len = parameters["max_len"]
    epochs = parameters["epochs"]
    r = parameters.get("r", 16)
    alpha_r = parameters.get("alpha_r", 16)
    learning_rate = parameters.get("learning_rate", 2e-4)
    seed = parameters.get("seed", 42)
    warmup_ratio = parameters.get("warmup_ratio", 0.06)
    
    # è®¾ç½®éšæœºç§å­
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    os.environ['PYTHONHASHSEED'] = str(seed)
    os.environ['TRANSFORMERS_SEED'] = str(seed)
    print(f"âœ… å·²è®¾ç½®éšæœºç§å­: {seed} (ç¡®ä¿ç»“æœå¯é‡å¤)")

    print("\n" + "="*60)
    print(f"QST (è®ºæ–‡å®ç°) 4-bité‡åŒ–è®­ç»ƒ: {task}")
    print(f"æ¨¡å‹: {model_checkpoint}, ä¾§ç½‘ç»œr: {r}, Downsamplerç§©: {alpha_r}, ç§å­: {seed}")
    print("="*60 + "\n")
    
    actual_task = "mnli" if task == "mnli-mm" else task
    dataset = load_dataset("nyu-mll/glue", actual_task)
    
    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True, trust_remote_code=True)
    num_labels = 3 if task.startswith("mnli") else 1 if task == "stsb" else 2
    
    # 4-bité‡åŒ–é…ç½®
    quant_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16
    )
    
    compute_device = "cuda:0" if torch.cuda.is_available() else "cpu"
    
    print(f"åŠ è½½4-bité‡åŒ–ä¸»ç½‘ç»œ (f) åˆ° {compute_device}: {model_checkpoint}")
    base_model_4bit = AutoModelForSequenceClassification.from_pretrained(
        model_checkpoint,
        quantization_config=quant_config,
        torch_dtype=torch.bfloat16,
        num_labels=num_labels,
        device_map=compute_device,
        trust_remote_code=True,
        attn_implementation="eager"
    )
    
    if tokenizer.pad_token is None:
        tokenizer.add_special_tokens({"pad_token": DEFAULT_PAD_TOKEN})
        base_model_4bit.resize_token_embeddings(len(tokenizer))
    
    base_model_4bit.config.pad_token_id = tokenizer.pad_token_id
    
    # åˆ›å»ºQSTåŒ…è£…æ¨¡å‹
    print("åˆ›å»º QST åŒ…è£…æ¨¡å‹ (f + g)...")
    model = QSTLlamaForSequenceClassification(
        config=base_model_4bit.config,
        base_model_4bit=base_model_4bit,
        reduction_factor_r=r,
        adapter_rank_r=alpha_r
    )
    # å…³é”®: ç¡®ä¿æ‰€æœ‰QSTç»„ä»¶éƒ½æ˜¯bfloat16
    model = model.to(compute_device, dtype=torch.bfloat16)

    print_trainable_parameters(model)
    
    # æ•°æ®é¢„å¤„ç†
    sentence1_key, sentence2_key = task_to_keys[task]
    
    def preprocess_function(examples):
        args = (examples[sentence1_key],) if sentence2_key is None else (examples[sentence1_key], examples[sentence2_key])
        return tokenizer(*args, truncation=True, max_length=max_len, padding="max_length")
    
    print("æ•°æ®é¢„å¤„ç†...")
    encoded_dataset = dataset.map(preprocess_function, batched=True)
    
    validation_key = "validation_mismatched" if task == "mnli-mm" else "validation_matched" if task == "mnli" else "validation"
    
    # è®­ç»ƒ
    metric_name = "pearson" if task == "stsb" else "matthews_correlation" if task == "cola" else "accuracy"
    args = TrainingArguments(
        f"llama3-qst-4bit-{task}",
        eval_strategy="epoch",
        save_strategy="epoch",  # ä¸´æ—¶ç¦ç”¨ä¿å­˜
        lr_scheduler_type="cosine",
        learning_rate=learning_rate,
        warmup_ratio=warmup_ratio,
        per_device_train_batch_size=batch_size,
        per_device_eval_batch_size=batch_size,
        num_train_epochs=epochs,
        weight_decay=0.01,  # é™ä½æ­£åˆ™åŒ–
        load_best_model_at_end=True,
        dataloader_num_workers=2,
        metric_for_best_model=metric_name,
        push_to_hub=False,
        fp16=False,
        bf16=True,
        logging_steps=50,
        save_total_limit=2,
        max_grad_norm=1,  
        seed=seed,
        data_seed=seed,
        report_to="none",
    )
    
    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
    
    trainer = QSTTrainer(
        model,
        args,
        train_dataset=encoded_dataset["train"],
        eval_dataset=encoded_dataset[validation_key],
        tokenizer=tokenizer,
        data_collator=data_collator,
        compute_metrics=lambda x: compute_metrics_sklearn(task, x)
    )
    
    print("ğŸš€ å¼€å§‹ QST è®­ç»ƒ...")
    if torch.cuda.is_available():
        torch.cuda.reset_peak_memory_stats()
    trainer.train()
    peak_memory_gb = 0
    if torch.cuda.is_available():
        peak_memory_gb = torch.cuda.max_memory_allocated() / 1024**3
    
    # âœ… è®­ç»ƒç»“æŸåï¼Œload_best_model_at_end=True å·²è‡ªåŠ¨åŠ è½½æœ€ä½³ checkpoint
    print("\nğŸ“ˆ è¯„ä¼°æœ€ä½³æ¨¡å‹ (å·²è‡ªåŠ¨åŠ è½½ best checkpoint)...")
    final_metrics = trainer.evaluate()
    final_metrics["peak_memory_gb"] = peak_memory_gb
    final_metrics["trainable_params"] = sum(p.numel() for p in model.parameters() if p.requires_grad)
    final_metrics["total_params"] = sum(p.numel() for p in model.parameters())
    final_metrics["trainable_ratio"] = (final_metrics["trainable_params"] / final_metrics["total_params"]) * 100
    return final_metrics

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="QST (è®ºæ–‡å®ç°) + 4-bité‡åŒ–è®­ç»ƒ")
    parser.add_argument("--model_checkpoint", type=str, default="meta-llama/Llama-3.2-1B")
    parser.add_argument("--batch_size", type=int, default=8)
    parser.add_argument("--max_len", type=int, default=512)
    parser.add_argument("--epochs", type=int, default=3)
    parser.add_argument("--task", type=str, default="sst2", help=f"GLUE ä»»åŠ¡: {list(task_to_keys.keys())}")
    parser.add_argument("--r", type=int, default=16, help="ä¾§ç½‘ç»œç¼©å‡å› å­ (è®ºæ–‡é»˜è®¤16)")
    parser.add_argument("--alpha_r", type=int, default=16, help="Downsampler é€‚é…å™¨ç§© (è®ºæ–‡é»˜è®¤16)")
    parser.add_argument("--seed", type=int, default=68, help="éšæœºç§å­ (ç¡®ä¿ç»“æœå¯é‡å¤)")
    
    args = parser.parse_args()
    
    parameters = {
        "model_checkpoint": args.model_checkpoint,
        "batch_size": args.batch_size,
        "max_len": args.max_len,
        "epochs": args.epochs,
        "r": args.r,
        "alpha_r": args.alpha_r,
        "seed": args.seed,
    }
    
    tasks = [args.task]
    
    results = {}
    for task in tasks:
        try:
            results[task] = train_qst_model(task, parameters)
            print(f"\nâœ… ä»»åŠ¡ {task} è®­ç»ƒæˆåŠŸ!")
            print(f"   å‡†ç¡®ç‡: {results[task].get('eval_accuracy', results[task].get('eval_pearson', 'N/A'))}")
        except Exception as e:
            print(f"\nâŒ ä»»åŠ¡ {task} è®­ç»ƒå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            
    print("\n" + "="*60)
    print("è®­ç»ƒå®Œæˆ! ç»“æœ:")
    print("="*60)
    for task, result in results.items():
        print(f"{task}: {result}")
